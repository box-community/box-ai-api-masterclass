{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97e7893c-2f06-49bf-a3a8-29162aae94ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to BoxWorks Master Class\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to BoxWorks Master Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb55adf-bb64-4799-b472-e5a136a869af",
   "metadata": {},
   "source": [
    "# Deep dive into Box AI API\n",
    "\n",
    "Prerequisites: \n",
    "1. Enable Box AI in Admin Console\n",
    "2. Create a Box Platform app\n",
    "3. Enable Manage AI scope in app's configuration tab\n",
    "4. Generate a developer token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d38a1aa-6e47-48b7-a3b4-04e817873b69",
   "metadata": {},
   "source": [
    "## Install Box Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81a0a628-cb36-4bfc-9653-73e4506d290f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: box-sdk-gen in /opt/homebrew/Cellar/jupyterlab/4.4.5/libexec/lib/python3.13/site-packages (1.16.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Cellar/jupyterlab/4.4.5/libexec/lib/python3.13/site-packages (from box-sdk-gen) (2.32.4)\n",
      "Requirement already satisfied: requests-toolbelt in /opt/homebrew/Cellar/jupyterlab/4.4.5/libexec/lib/python3.13/site-packages (from box-sdk-gen) (1.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/Cellar/jupyterlab/4.4.5/libexec/lib/python3.13/site-packages (from requests->box-sdk-gen) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Cellar/jupyterlab/4.4.5/libexec/lib/python3.13/site-packages (from requests->box-sdk-gen) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Cellar/jupyterlab/4.4.5/libexec/lib/python3.13/site-packages (from requests->box-sdk-gen) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/opt/certifi/lib/python3.13/site-packages (from requests->box-sdk-gen) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install box-sdk-gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0fd3c-44e2-426a-b8e6-530827097a5b",
   "metadata": {},
   "source": [
    "## Authenticate with a developer token and the get current user ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "35380441-b0b2-4e35-a915-1fb7d6882f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My user ID is 21278765957\n"
     ]
    }
   ],
   "source": [
    "from box_sdk_gen import BoxClient, BoxDeveloperTokenAuth\n",
    "\n",
    "auth = BoxDeveloperTokenAuth(token=\"TOKEN\")\n",
    "client = BoxClient(auth=auth)\n",
    "\n",
    "me = client.users.get_user_me()\n",
    "print(f\"My user ID is {me.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c0e348-de51-47ce-a6ce-f04b55bce35c",
   "metadata": {},
   "source": [
    "## Ask AI to summarise the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85077686-875a-4396-85ae-6a3fa24abb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from box_sdk_gen import CreateAiAskMode, AiItemAsk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "892924d7-d892-4c61-a81e-da714d60c0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'box_sdk_gen.schemas.ai_response_full.AiResponseFull'> {'answer': 'This document presents a novel process-centric taxonomy for organizing, classifying, and synthesizing tactile robot manipulation skills, addressing the challenge of scalable and reliable skill deployment in robotics. The authors introduce a formal framework that maps expert-defined process specifications to tactile skills through a hierarchical taxonomy, enabling efficient selection and learning of manipulation policies tailored to specific tasks such as insertion, cutting, and assembly. They implement this approach using the Graph-Guided Twist-Wrench Policy (GGTWreP) framework on a 7-DoF Franka Emika robot with a two-finger gripper, demonstrating robust performance across 28 industrially relevant skills with near 100% success rates even under disturbances. The framework integrates control, learning, and tactile feedback, allowing parameter adaptation and transferability between related skills. Compared to deep reinforcement learning methods like DDPG, their approach requires significantly less computational energy, making it suitable for real-world continuous learning scenarios. The paper discusses limitations such as manual policy design and scope restricted to rigid manipulators but outlines future directions including automation via large language models, extension to soft materials, and more complex bimanual tasks. Overall, the work lays foundational steps toward systematic robotic curricula based on established human vocational training standards, aiming to facilitate broader industrial automation with tactile robots.\\n\\nWould you like me to also organize this summary into key sections or highlight specific experimental results?', 'created_at': '2025-08-06T07:21:41.380000-07:00', 'completion_reason': 'done', 'ai_agent_info': {'models': [{'name': 'azure__openai__gpt_4_1_mini', 'provider': 'azure'}], 'processor': 'basic_text'}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.ai.create_ai_ask(\n",
    "    CreateAiAskMode.SINGLE_ITEM_QA,\n",
    "    \"summarize this document\",\n",
    "    [\n",
    "        AiItemAsk(\n",
    "            id=\"1925154254074\",\n",
    "            type=\"file\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27e11c4-4b6b-45ee-97b2-8a3806ce26ec",
   "metadata": {},
   "source": [
    "## Display the AI response in a readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "64b210a3-ec27-4b93-a865-1ede79fe7ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_ai_response(response):\n",
    "    \"\"\"\n",
    "    Display the AI response in a readable format\n",
    "    \"\"\"\n",
    "    # Extract the data from the response object\n",
    "    answer = response.answer if hasattr(response, 'answer') else \"No answer available\"\n",
    "    created_at = response.created_at if hasattr(response, 'created_at') else \"Unknown\"\n",
    "    completion_reason = response.completion_reason if hasattr(response, 'completion_reason') else \"Unknown\"\n",
    "    \n",
    "    # Print in a readable format\n",
    "    print(\"=== AI RESPONSE ===\")\n",
    "    print(answer)\n",
    "    print(f\"\\nGenerated at: {created_at}\")\n",
    "    print(f\"Completion status: {completion_reason}\")\n",
    "    \n",
    "    # Get model info if available\n",
    "    if hasattr(response, 'ai_agent_info') and hasattr(response.ai_agent_info, 'models'):\n",
    "        if response.ai_agent_info.models:\n",
    "            print(f\"AI model used: {response.ai_agent_info.models[0].name}\")\n",
    "\n",
    "summary = client.ai.create_ai_ask(\n",
    "    CreateAiAskMode.SINGLE_ITEM_QA,\n",
    "    \"Summarize this document\",\n",
    "    [\n",
    "        AiItemAsk(\n",
    "            id=\"1925154254074\",\n",
    "            type=\"file\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "display_ai_response(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76331991-a46d-4365-8ee2-3de55f43982c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AI RESPONSE ===\n",
      "This document presents a novel process-centric taxonomy for organizing, classifying, and synthesizing tactile robot manipulation skills to improve the deployment and learning of autonomous robotic manipulation in industrial settings. The authors introduce a formal framework that links expert-defined process specifications to tactile skill implementations via a hierarchical taxonomy, enabling scalable and efficient skill selection and learning. They validate their approach experimentally on 28 diverse real-world manipulation tasks using the Graph-Guided Twist-Wrench Policy (GGTWreP) framework with a standard 7-DoF manipulator, achieving near 100% success rates and robust performance under disturbances. The work emphasizes integrating control, learning, and process constraints, reducing reliance on robotics expertise by leveraging established human vocational curricula as a knowledge base. Compared to deep reinforcement learning methods, their approach is more energy-efficient and sample-efficient. Limitations include manual policy design and focus on rigid two-finger grippers, with future directions toward automation via large language models, extension to soft materials, and broader task domains. Overall, this taxonomy offers a systematic foundation for creating comprehensive robot manipulation curricula and practical skill libraries for industrial automation.\n",
      "\n",
      "Generated at: 2025-08-12 02:54:59.597000-07:00\n",
      "Completion status: done\n",
      "AI model used: azure__openai__gpt_4_1_mini\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b81c4eb1-326e-4db1-aec4-96fad649809b",
   "metadata": {},
   "source": [
    "## Query a different content type: image\n",
    "\n",
    "We'll create a simple function that determines if given file is a passport page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "983ce1cb-5d13-461c-a452-8cc013fe430f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AI RESPONSE ===\n",
      "YES\n",
      "\n",
      "Generated at: 2025-08-12 04:20:07.009000-07:00\n",
      "Completion status: done\n",
      "AI model used: azure__openai__gpt_4_1_mini\n",
      "=== AI RESPONSE ===\n",
      "NO\n",
      "\n",
      "Generated at: 2025-08-12 04:20:09.271000-07:00\n",
      "Completion status: done\n",
      "AI model used: azure__openai__gpt_4_1_mini\n"
     ]
    }
   ],
   "source": [
    "def is_passport(file_id):\n",
    "    response = client.ai.create_ai_ask(\n",
    "        CreateAiAskMode.SINGLE_ITEM_QA,\n",
    "        \"Is this document a passport page? Return only YES or NO\",\n",
    "        [\n",
    "            AiItemAsk(\n",
    "                id=file_id,\n",
    "                type=\"file\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "display_ai_response(is_passport(\"1947306147694\"))\n",
    "display_ai_response(is_passport(\"1947326375742\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3686408-541d-4283-9189-48ea8f7df601",
   "metadata": {},
   "source": [
    "## Translate document with Box AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "40824cd1-7bfa-472a-a1a1-f985677b7721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AI RESPONSE ===\n",
      "Please provide the document you want me to translate into Polish.\n",
      "\n",
      "Generated at: 2025-08-12 03:52:12.017000-07:00\n",
      "Completion status: done\n",
      "AI model used: azure__openai__gpt_4_1_mini\n"
     ]
    }
   ],
   "source": [
    "Need to rewrite to ask endpoint\n",
    "# from box_sdk_gen import CreateAiTextGenItems, CreateAiTextGenItemsTypeField\n",
    "\n",
    "# def translate_document(file_id, language):\n",
    "#     response = client.ai.create_ai_text_gen(\n",
    "#         f\"\"\"Translate this document to {language}\"\"\",\n",
    "#         [\n",
    "#             CreateAiTextGenItems(\n",
    "#                 id=file_id,\n",
    "#                 type=CreateAiTextGenItemsTypeField.FILE,\n",
    "#             )\n",
    "#         ],\n",
    "#     )\n",
    "#     return response\n",
    "\n",
    "# display_ai_response(translate_document(\"1925154254074\", \"Polish\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b940b50-5ce6-4254-bf1c-2fc9b4ff7eb6",
   "metadata": {},
   "source": [
    "## Get default AI API configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6202e683-6fd4-438b-b186-23900441b8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'box_sdk_gen.schemas.ai_agent_ask.AiAgentAsk'> {'type': 'ai_agent_ask', 'long_text': {'model': 'azure__openai__gpt_4_1_mini', 'num_tokens_for_completion': 6000, 'llm_endpoint_params': {'type': 'openai_params', 'temperature': 0, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 1.5, 'stop': '<|im_end|>'}, 'prompt_template': 'Current date: {current_date}\\n\\nI will ask you for help and provide subsections of one document delimited by five backticks (`````) at the beginning and at the end.\\nIf I make a reference to \"this\", I am referring to the document I provided between the five backticks. I may ask you a question where the answer is contained within the document.  In that case, do your best to answer using only the document, but if you cannot, feel free to mention that you couldn\\'t find an answer in the document, but you have some answer from your general knowledge.\\nI may ask you to perform some kind of computation or symbol manipulation such as filtering a list, counting something, summing, averaging, and other aggregation/grouping functions or some combination of them.  In these cases, first list the plan of how you plan to perform such a computation, then follow that plan step by step, keeping track of intermediate results, and at the end tell me the final answer.\\nI may ask you to enumerate or somehow list people, places, characters, or other important things from the document, if I do so, please only use the document provided to list them.\\n\\nFollow-Up Question Logic: After answering the question, do the following:\\n1. Determine if the question requires further clarifications\\n2. Determine if the answer needs further analysis or follow-ups to help the user based on the original question\\n\\nIf answer to 1. is Yes then suggest potential clarifications to the user\\'s original question\\nIf answer to 2. is Yes then suggest one best possible follow-up question based on the user\\'s question and the answer returned\\nIf answer to both 1. and 2. is Yes then do 1. and NOT 2.\\nIf answer to both 1. and 2. are No then do NOT return any follow up questions or suggestions\\n\\nNote: The current UI does not support displaying images generated by the model such as charts or wireframes, so avoid suggesting generating these. You can suggest tabulating, generating lists, or writing code.\\n\\nExample Follow-Up: \"Would you like me to also organize this into a simple table or framework (like \\'What they did\\' → \\'Leader mindset demonstrated\\')? It might be helpful if you\\'re writing a formal review or nomination!\"\\n\\nTEXT FROM DOCUMENT STARTS\\n`````\\n{content}\\n`````\\nTEXT FROM DOCUMENT ENDS\\n{knowledge_content}\\nNever mention five backticks in your response. Unless you are told otherwise, a one paragraph response is sufficient for any requested summarization tasks.\\nHere is how I need help from you: {user_question}', 'embeddings': {'model': 'azure__openai__text_embedding_ada_002', 'strategy': {'id': 'basic', 'num_tokens_per_chunk': 128}}}, 'basic_text': {'model': 'azure__openai__gpt_4_1_mini', 'num_tokens_for_completion': 6000, 'llm_endpoint_params': {'type': 'openai_params', 'temperature': 0, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 1.5, 'stop': '<|im_end|>'}, 'prompt_template': 'Current date: {current_date}\\n\\nI will ask you for help and provide the entire text of one document delimited by five backticks (`````) at the beginning and at the end.\\nIf I make a reference to \"this\", I am referring to the document I provided between the five backticks. I may ask you a question where the answer is contained within the document.  In that case, do your best to answer using only the document, but if you cannot, feel free to mention that you couldn\\'t find an answer in the document, but you have some answer from your general knowledge.\\nI may ask you to perform some kind of computation or symbol manipulation such as filtering a list, counting something, summing, averaging, and other aggregation/grouping functions or some combination of them.  In these cases, first list the plan of how you plan to perform such a computation, then follow that plan step by step, keeping track of intermediate results, and at the end tell me the final answer.\\nI may ask you to enumerate or somehow list people, places, characters, or other important things from the document, if I do so, please only use the document provided to list them.\\nI may ask you to translate content - please translate each element and section completely, maintaining all structure and formatting. By default treat all translation requests as comprehensive, line-by-line translations.\\n\\nFollow-Up Question Logic: After answering the question, do the following:\\n1. Determine if the question requires further clarifications\\n2. Determine if the answer needs further analysis or follow-ups to help the user based on the original question\\n\\nIf answer to 1. is Yes then suggest potential clarifications to the user\\'s original question\\nIf answer to 2. is Yes then suggest one best possible follow-up question based on the user\\'s question and the answer returned\\nIf answer to both 1. and 2. is Yes then do 1. and NOT 2.\\nIf answer to both 1. and 2. are No then do NOT return any follow up questions or suggestions\\n\\nNote: The current UI does not support displaying images generated by the model such as charts or wireframes, so avoid suggesting generating these. You can suggest tabulating, generating lists, or writing code.\\n\\nExample Follow-Up: \"Would you like me to also organize this into a simple table or framework (like \\'What they did\\' → \\'Leader mindset demonstrated\\')? It might be helpful if you\\'re writing a formal review or nomination!\"\\n\\nTEXT FROM DOCUMENT STARTS\\n`````\\n{content}\\n`````\\nTEXT FROM DOCUMENT ENDS\\n{knowledge_content}\\nNever mention five backticks in your response. Unless you are told otherwise, a one paragraph response is sufficient for any requested summarization tasks.\\nHere is how I need help from you: {user_question}'}, 'spreadsheet': {'model': 'azure__openai__gpt_4_1_mini', 'num_tokens_for_completion': 1, 'llm_endpoint_params': {'type': 'openai_params', 'temperature': 0, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'stop': '<|im_end|>'}}, 'long_text_multi': {'model': 'azure__openai__gpt_4_1_mini', 'num_tokens_for_completion': 6000, 'llm_endpoint_params': {'type': 'openai_params', 'temperature': 0, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 1.5, 'stop': '<|im_end|>'}, 'system_message': 'Role and Goal: You are an assistant designed to analyze and answer questions based on provided snippets from multiple documents, which can include business-oriented documents like docs, presentations, PDFs, etc. You will respond concisely using only the information from the provided documents, then determine if follow-up questions would be helpful.\\n\\nCore Response Constraints: Avoid engaging in chatty or extensive conversational interactions and focus on providing direct answers. When the answer is contained within the documents, use only the document information. If you cannot find an answer in the provided documents, mention that you couldn\\'t find an answer in the documents, but you may provide relevant information from your general knowledge.\\n\\nComputational Tasks: If asked to perform calculations, filtering, counting, summing, averaging, or other aggregation functions:\\n1. First list your plan for performing the computation\\n2. Follow that plan step by step, keeping track of intermediate results\\n3. Provide the final answer clearly at the end\\n\\nEnumeration Tasks: When asked to list people, places, entities, or other important elements, use only the information from the provided documents.\\n\\nResponse Length: Unless specified otherwise, one paragraph is sufficient for summarization tasks.\\n\\nFollow-Up Question Logic: After answering the user\\'s question, do the following:\\n1. Determine if the question requires further clarifications\\n2. Determine if the answer needs further analysis or follow-ups to help the user based on the original question\\n\\nIf answer to 1. is Yes then suggest potential clarifications to the user\\'s original question\\nIf answer to 2. is Yes then suggest one best possible follow-up question based on the user\\'s question and the answer returned\\nIf answer to both 1. and 2. is Yes then do 1. and NOT 2.\\nIf answer to both 1. and 2. are No then do NOT return any follow up questions or suggestions\\n\\nNote: The current UI does not support displaying images generated by the model such as charts or wireframes, so avoid suggesting generating these. You can suggest tabulating, generating lists, or writing code.\\n\\nExample Follow-Up: \"Would you like me to also organize this into a simple table or framework (like \\'What they did\\' → \\'Leader mindset demonstrated\\')? It might be helpful if you\\'re writing a formal review or nomination!\"\\n\\nGuidelines: When answering, consider the file\\'s name and path to assess relevance to the question. In cases of conflicting information from multiple documents, list the different answers with citations. Consider the current date to be the date given.\\n\\nPersonalization: Maintain a professional tone for document analysis, but use a helpful, first-person approach when suggesting follow-up questions or clarifications.', 'prompt_template': 'Current date: {current_date}\\n\\nTEXT FROM DOCUMENTS STARTS\\n{content}\\nTEXT FROM DOCUMENTS ENDS\\n\\n{knowledge_content}\\n\\nHere is how I need help from you: {user_question}', 'embeddings': {'model': 'azure__openai__text_embedding_ada_002', 'strategy': {'id': 'basic', 'num_tokens_per_chunk': 128}}}, 'basic_text_multi': {'model': 'azure__openai__gpt_4_1_mini', 'num_tokens_for_completion': 6000, 'llm_endpoint_params': {'type': 'openai_params', 'temperature': 0, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 1.5, 'stop': '<|im_end|>'}, 'prompt_template': 'Current date: {current_date}\\n\\nTEXT FROM DOCUMENTS STARTS\\n{content}\\nTEXT FROM DOCUMENTS ENDS\\n\\n{knowledge_content}\\n\\nHere is how I need help from you: {user_question}'}, 'basic_image': {'model': 'azure__openai__gpt_4_1_mini', 'num_tokens_for_completion': 8400, 'llm_endpoint_params': {'type': 'openai_params', 'temperature': 0, 'top_p': 1, 'frequency_penalty': 0.75, 'presence_penalty': 0.75, 'stop': '<|im_end|>'}, 'prompt_template': 'Current date: {current_date}\\n\\nI will ask you for help and provide document that contain images. If I make a reference to \"this\", I am referring to the image I provided. I may ask you a question where the answer is contained within the image.  In that case, do your best to answer using only the image, but if you cannot, feel free to mention that you couldn\\'t find an answer in the image, but you have some answer from your general knowledge.\\nI may ask you to perform some kind of computation or symbol manipulation such as filtering a list, counting something, summing, averaging, and other aggregation/grouping functions or some combination of them.  In these cases, first list the plan of how you plan to perform such a computation, then follow that plan step by step, keeping track of intermediate results, and at the end tell me the final answer.\\nI may ask you to enumerate or somehow list people, places, characters, or other important things from the image, if I do so, please only use the image provided to list them.\\nFollow these steps to extract information provided in the image\\n1. Describe the overall image content and type\\n2. Identify key visual elements (objects, text, people, graphics)\\n3. Note relationships and patterns between elements\\n4. Extract relevant details based on the question\\n5. Provide a clear, evidence-based answer\\nExamine all text visible in the image\\n1. Read and transcribe any text, signs, or numbers\\n2. Note the location and context of each text element\\n3. Consider the text style, font, and formatting\\n4. Identify any logos or branded text\\n5. Note any partially visible or obscured text\\n6. Consider the relationship between text and other visual elements\\nTEXT FROM DOCUMENT STARTS\\n`````\\n{content}\\n`````\\nTEXT FROM DOCUMENT ENDS\\n{knowledge_content}\\nNever mention five backticks in your response. Unless you are told otherwise, a one paragraph response is sufficient for any requested summarization tasks.\\nHere is how I need help from you: {user_question}'}, 'basic_image_multi': {'model': 'azure__openai__gpt_4_1_mini', 'num_tokens_for_completion': 8400, 'llm_endpoint_params': {'type': 'openai_params', 'temperature': 0, 'top_p': 1, 'frequency_penalty': 0.75, 'presence_penalty': 0.75, 'stop': '<|im_end|>'}, 'prompt_template': 'Current date: {current_date}\\n\\nI will ask you for help and provide document that contain images. If I make a reference to \"this\", I am referring to the image I provided. I may ask you a question where the answer is contained within the image.  In that case, do your best to answer using only the image, but if you cannot, feel free to mention that you couldn\\'t find an answer in the image, but you have some answer from your general knowledge.\\nI may ask you to perform some kind of computation or symbol manipulation such as filtering a list, counting something, summing, averaging, and other aggregation/grouping functions or some combination of them.  In these cases, first list the plan of how you plan to perform such a computation, then follow that plan step by step, keeping track of intermediate results, and at the end tell me the final answer.\\nI may ask you to enumerate or somehow list people, places, characters, or other important things from the image, if I do so, please only use the image provided to list them.\\nFollow these steps to extract information provided in the image\\n1. Describe the overall image content and type\\n2. Identify key visual elements (objects, text, people, graphics)\\n3. Note relationships and patterns between elements\\n4. Extract relevant details based on the question\\n5. Provide a clear, evidence-based answer\\nExamine all text visible in the image\\n1. Read and transcribe any text, signs, or numbers\\n2. Note the location and context of each text element\\n3. Consider the text style, font, and formatting\\n4. Identify any logos or branded text\\n5. Note any partially visible or obscured text\\n6. Consider the relationship between text and other visual elements\\nTEXT FROM DOCUMENT STARTS\\n`````\\n{content}\\n`````\\nTEXT FROM DOCUMENT ENDS\\n{knowledge_content}\\nNever mention five backticks in your response. Unless you are told otherwise, a one paragraph response is sufficient for any requested summarization tasks.\\nHere is how I need help from you: {user_question}'}}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from box_sdk_gen import GetAiAgentDefaultConfigMode\n",
    "\n",
    "client.ai.get_ai_agent_default_config(GetAiAgentDefaultConfigMode.ASK, language=\"en-US\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb8a73-5e3e-469d-b6a9-d6202d4e679c",
   "metadata": {},
   "source": [
    "## Format config output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "96c3fe76-38e7-4fa1-aa10-2e0de6a70520",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "BoxSDKError",
     "evalue": "\nTimestamp: 2025-08-12 15:31:48.811358\nUnderlying error: None\nMessage: Developer token has expired. Please provide a new one.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBoxSDKError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     71\u001b[39m                 print_field(\u001b[33m\"\u001b[39m\u001b[33m  Tokens per chunk\u001b[39m\u001b[33m\"\u001b[39m, safe_getattr(strategy, \u001b[33m'\u001b[39m\u001b[33mnum_tokens_per_chunk\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== END AI CONFIG ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m display_ai_config(\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_ai_agent_default_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGetAiAgentDefaultConfigMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mASK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43men-US\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.5/libexec/lib/python3.13/site-packages/box_sdk_gen/managers/ai.py:356\u001b[39m, in \u001b[36mAiManager.get_ai_agent_default_config\u001b[39m\u001b[34m(self, mode, language, model, extra_headers)\u001b[39m\n\u001b[32m    348\u001b[39m query_params_map: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m] = prepare_params(\n\u001b[32m    349\u001b[39m     {\n\u001b[32m    350\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m'\u001b[39m: to_string(mode),\n\u001b[32m   (...)\u001b[39m\u001b[32m    353\u001b[39m     }\n\u001b[32m    354\u001b[39m )\n\u001b[32m    355\u001b[39m headers_map: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m] = prepare_params({**extra_headers})\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m response: FetchResponse = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnetwork_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnetwork_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mFetchOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnetwork_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbase_urls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/2.0/ai_agent_default\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mResponseFormat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mJSON\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnetwork_session\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnetwork_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m deserialize(\n\u001b[32m    370\u001b[39m     response.data,\n\u001b[32m    371\u001b[39m     Union[AiAgentAsk, AiAgentTextGen, AiAgentExtract, AiAgentExtractStructured],\n\u001b[32m    372\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.5/libexec/lib/python3.13/site-packages/box_sdk_gen/networking/box_network_client.py:116\u001b[39m, in \u001b[36mBoxNetworkClient.fetch\u001b[39m\u001b[34m(self, options)\u001b[39m\n\u001b[32m    113\u001b[39m     fetch_response = FetchResponse(status=\u001b[32m0\u001b[39m, headers={})\n\u001b[32m    115\u001b[39m attempt_nr += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m should_retry = \u001b[43mretry_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshould_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfetch_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfetch_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfetch_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattempt_number\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattempt_for_retry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m should_retry:\n\u001b[32m    123\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset_options_stream(\n\u001b[32m    124\u001b[39m         options, options_stream_position, response.raised_exception\n\u001b[32m    125\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.5/libexec/lib/python3.13/site-packages/box_sdk_gen/networking/retries.py:73\u001b[39m, in \u001b[36mBoxRetryStrategy.should_retry\u001b[39m\u001b[34m(self, fetch_options, fetch_response, attempt_number)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fetch_response.status == \u001b[32m401\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fetch_options.auth == \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[43mfetch_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrefresh_token\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnetwork_session\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfetch_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnetwork_session\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_successful:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.5/libexec/lib/python3.13/site-packages/box_sdk_gen/box/developer_token_auth.py:70\u001b[39m, in \u001b[36mBoxDeveloperTokenAuth.refresh_token\u001b[39m\u001b[34m(self, network_session)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrefresh_token\u001b[39m(\n\u001b[32m     63\u001b[39m     \u001b[38;5;28mself\u001b[39m, *, network_session: Optional[NetworkSession] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     64\u001b[39m ) -> AccessToken:\n\u001b[32m     65\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[33;03m    Developer token cannot be refreshed\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33;03m    :param network_session: An object to keep network session state, defaults to None\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[33;03m    :type network_session: Optional[NetworkSession], optional\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BoxSDKError(\n\u001b[32m     71\u001b[39m         message=\u001b[33m'\u001b[39m\u001b[33mDeveloper token has expired. Please provide a new one.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     72\u001b[39m     )\n",
      "\u001b[31mBoxSDKError\u001b[39m: \nTimestamp: 2025-08-12 15:31:48.811358\nUnderlying error: None\nMessage: Developer token has expired. Please provide a new one."
     ]
    }
   ],
   "source": [
    "def display_ai_config(response):\n",
    "    \"\"\"Display the AI config in a readable format\"\"\"\n",
    "    \n",
    "    def safe_getattr(obj, attr, default=None):\n",
    "        \"\"\"Safely get attribute with default value\"\"\"\n",
    "        return getattr(obj, attr, default) if hasattr(obj, attr) else default\n",
    "    \n",
    "    def print_field(label, value, truncate_at=100):\n",
    "        \"\"\"Print field with optional truncation\"\"\"\n",
    "        if value is None:\n",
    "            return\n",
    "        value_str = str(value)\n",
    "        if len(value_str) > truncate_at:\n",
    "            print(f\"{label}: {value_str[:truncate_at]}...\")\n",
    "        else:\n",
    "            print(f\"{label}: {value_str}\")\n",
    "    \n",
    "    # Header\n",
    "    print(\"=== AI CONFIG ===\")\n",
    "    print(f\"\\nAgent type: {safe_getattr(response, 'type', 'No agent type available')}\")\n",
    "    \n",
    "    # Model types to check\n",
    "    model_types = [\n",
    "        'long_text', 'basic_text', 'spreadsheet', 'long_text_multi',\n",
    "        'basic_text_multi', 'basic_image', 'basic_image_multi'\n",
    "    ]\n",
    "    \n",
    "    # LLM parameter mappings\n",
    "    llm_params_map = {\n",
    "        'type': 'Type',\n",
    "        'temperature': 'Temperature',\n",
    "        'top_p': 'Top-p',\n",
    "        'frequency_penalty': 'Frequency penalty',\n",
    "        'presence_penalty': 'Presence penalty',\n",
    "        'stop': 'Stop sequence'\n",
    "    }\n",
    "    \n",
    "    for model_type in model_types:\n",
    "        model_config = safe_getattr(response, model_type)\n",
    "        if not model_config:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n--- {model_type.upper().replace('_', ' ')} ---\")\n",
    "        \n",
    "        # Basic fields\n",
    "        print_field(\"Model\", safe_getattr(model_config, 'model'))\n",
    "        print_field(\"Max tokens\", safe_getattr(model_config, 'num_tokens_for_completion'))\n",
    "        \n",
    "        # LLM parameters\n",
    "        llm_params = safe_getattr(model_config, 'llm_endpoint_params')\n",
    "        if llm_params:\n",
    "            print(\"LLM Parameters:\")\n",
    "            for attr, label in llm_params_map.items():\n",
    "                value = safe_getattr(llm_params, attr)\n",
    "                if value is not None:\n",
    "                    print(f\"  {label}: {value}\")\n",
    "        \n",
    "        # System message and prompt template\n",
    "        print_field(\"System message\", safe_getattr(model_config, 'system_message'))\n",
    "        print_field(\"Prompt template\", safe_getattr(model_config, 'prompt_template'))\n",
    "        \n",
    "        # Embeddings configuration\n",
    "        embeddings = safe_getattr(model_config, 'embeddings')\n",
    "        if embeddings:\n",
    "            print(\"Embeddings:\")\n",
    "            print_field(\"  Model\", safe_getattr(embeddings, 'model'))\n",
    "            \n",
    "            strategy = safe_getattr(embeddings, 'strategy')\n",
    "            if strategy:\n",
    "                print_field(\"  Strategy\", safe_getattr(strategy, 'id'))\n",
    "                print_field(\"  Tokens per chunk\", safe_getattr(strategy, 'num_tokens_per_chunk'))\n",
    "    \n",
    "    print(\"\\n=== END AI CONFIG ===\")\n",
    "\n",
    "display_ai_config(client.ai.get_ai_agent_default_config(GetAiAgentDefaultConfigMode.ASK, language=\"en-US\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ccbb6a-97d1-4312-876b-84c6783400ab",
   "metadata": {},
   "source": [
    "## Get default config for text gen endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "613d69ce-f3c1-49e4-b8b8-d85d4796f85d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'box_sdk_gen.schemas.ai_agent_text_gen.AiAgentTextGen'> {'type': 'ai_agent_text_gen', 'basic_gen': {'model': 'azure__openai__gpt_4_1_mini', 'num_tokens_for_completion': 12000, 'llm_endpoint_params': {'type': 'openai_params', 'temperature': 0.1, 'top_p': 1, 'frequency_penalty': 0.75, 'presence_penalty': 0.75, 'stop': '<|im_end|>'}, 'system_message': '\\nIf you need to know today\\'s date to respond, it is {current_date}.\\nThe user is working in a collaborative document creation editor called Box Notes.\\nIf the user simply asks to \"improve\" the text, then simplify the language and remove jargon, unless the user specifies otherwise.\\nTry your best to format the document nicely using Markdown.  When you use headers, make them prominent. Be comprehensive in your answer. When asked to create or modify lists, similar formatting requests, provide the list of content without adding titles, headers, introductory text, or concluding explanations unless specifically requested.\\n', 'prompt_template': '{user_question}', 'embeddings': {'model': 'azure__openai__text_embedding_ada_002', 'strategy': {'id': 'basic', 'num_tokens_per_chunk': 128}}, 'content_template': '{content}{knowledge_content}'}}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.ai.get_ai_agent_default_config(GetAiAgentDefaultConfigMode.TEXT_GEN, language=\"en-US\")\n",
    "\n",
    "# display_ai_config(client.ai.get_ai_agent_default_config(GetAiAgentDefaultConfigMode.TEXT_GEN, language=\"en-US\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c179bef-e9db-429b-b5a5-baa5524f9e6a",
   "metadata": {},
   "source": [
    "## Overwrite default config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d38f16-f2ce-4110-bd72-7cbc65907c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffa5ebbc-2c7d-446e-b2ef-6243eefe3660",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "4. Generate summaries from files in one folder and generate an index Note. > what's the limit of number of files\n",
    "5. Query Box Hubs\n",
    "6. Query a spreadsheet\n",
    "7. Translate a document with multiple documents\n",
    "8. Free form extruct warious formats\n",
    "9. Extract structured + apply metadata template\n",
    "10. Use extract enhanced agent\n",
    "12. Show the JSON configurator\n",
    "13. Owerwrite the default model\n",
    "14. Talk about max tokens\n",
    "15. Define system prompt\n",
    "16. Show examples of temperature modification\n",
    "17. Create Studio AI agent\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
